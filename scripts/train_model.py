import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset, Value

# 1. –î–∞—Ç–∞—Å–µ—Ç—Ç—ñ –æ“õ—É
df = pd.read_csv('data/dataset_fixed.csv')

# üì¢ –ö–æ–ª–æ–Ω–∫–∞ –∞—Ç—Ç–∞—Ä—ã–Ω –¥“±—Ä—ã—Å—Ç–∞–π–º—ã–∑
df.columns = ['text', 'label']

# üõ† –¢–µ–∫—Å–µ—Ä—É “Ø—à—ñ–Ω –ø—Ä–∏–Ω—Ç “õ–æ—è–º—ã–∑
print(df.head())
print(df.columns)

# 2. –õ–µ–π–±–ª–¥–∞—Ä–¥—ã —Ü–∏—Ñ—Ä“ì–∞ –∞—É—ã—Å—Ç—ã—Ä—É
label2id = {
    '–≠–∫—Å—Ç—Ä–µ–º–∏–∑–º –∂–æ“õ': 0,
    '–ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —ç–∫—Å—Ç—Ä–µ–º–∏–∑–º': 1,
    '–ö—Å–µ–Ω–æ—Ñ–æ–±–∏—è': 2,
    '–†–µ–ª–∏–≥–∏–æ–∑–Ω—ã–π —ç–∫—Å—Ç—Ä–µ–º–∏–∑–º': 3,
}
id2label = {v: k for k, v in label2id.items()}

df['label'] = df['label'].map(label2id)

# 3. –¢–µ–∫ –¥“±—Ä—ã—Å –∂–∞–∑—ã–ª“ì–∞–Ω –º”ô–ª—ñ–º–µ—Ç—Ç–µ—Ä–¥—ñ “õ–∞–ª–¥—ã—Ä—É
df = df.dropna(subset=['text', 'label'])
df = df[df['label'].isin([0, 1, 2, 3])]
df = df.reset_index(drop=True)

# 4. Train/Test –±”©–ª—É
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42
)

# 5. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

# 6. –î–∞—Ç–∞—Å–µ—Ç—Ç–µ—Ä –∂–∞—Å–∞—É
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels
})
val_dataset = Dataset.from_dict({
    'input_ids': val_encodings['input_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': val_labels
})

train_dataset = train_dataset.cast_column('labels', Value('int64'))
val_dataset = val_dataset.cast_column('labels', Value('int64'))

# 7. –ú–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É
model = BertForSequenceClassification.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=4,
    id2label=id2label,
    label2id=label2id
)

# 8. –¢—Ä–µ–Ω–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç—Ç–µ—Ä
training_args = TrainingArguments(
    output_dir='./models/saved_model/',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=4,
    logging_dir='./logs',
    save_total_limit=1,
    save_strategy="epoch"
)


# 9. –¢—Ä–µ–Ω–µ—Ä –∂–∞—Å–∞—É
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# 10. Fine-tune –±–∞—Å—Ç–∞—É
trainer.train()

# 11. –ú–æ–¥–µ–ª—å–¥—ñ —Å–∞“õ—Ç–∞—É
model.save_pretrained('models/saved_model/')
tokenizer.save_pretrained('models/saved_model/')

print("‚úÖ Fine-tune —Å”ô—Ç—Ç—ñ –∞—è“õ—Ç–∞–ª–¥—ã –∂”ô–Ω–µ –º–æ–¥–µ–ª—å —Å–∞“õ—Ç–∞–ª–¥—ã!")
